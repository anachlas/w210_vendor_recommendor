{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering using Spark\n",
    "\n",
    "\n",
    "## TO-DO\n",
    "- Running collaborative filtering using Mllib on Spark\n",
    "    - Using implicit feedback (we do not have any direct input from the users regarding their preferences)\n",
    "- Running on DataProc on multiple clusters\n",
    "- Loading the data set from Bigquery\n",
    "- Writing the output into Biquery\n",
    "\n",
    "\n",
    "## References\n",
    "- https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\n",
    "- http://ieeexplore.ieee.org/document/4781121/\n",
    "- https://link.springer.com/chapter/10.1007%2F978-3-540-68880-8_32\n",
    "- https://cloud.google.com/dataproc/docs/guides/setup-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run collaborative filtering using MLlib - LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Jul  2 2016 17:43:17)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/ozimmer/GoogleDrive/berkeley/w261/spark-2.0.0-bin-hadoop2.6'\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 2670.12689742\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"/Users/ozimmer/GoogleDrive/berkeley/w210/w210_vendor_recommendor/test_spark_1.csv\")\n",
    "header = data.first() #filter out the header\n",
    "\n",
    "ratings = data.filter(lambda row: row != header)\\\n",
    "    .map(lambda l: l.split(','))\\\n",
    "    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "\n",
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(ratings, rank, numIterations)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/myCollaborativeFilter\")\n",
    "sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=143, product=348288, rating=469886.4711073065),\n",
       " Rating(user=143, product=306804, rating=-47573.151232822696),\n",
       " Rating(user=143, product=795221, rating=-57848.51871782569),\n",
       " Rating(user=143, product=795270, rating=-75898.31224407976)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a RDD for prediction\n",
    "data = [(145, 895988), (143, 348288), (143, 795270), (143, 795221), (143, 306804)]\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "#Paste the prediction results in the model\n",
    "model.predictAll(data_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-RDD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=long(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProc - submit a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py...\n",
      "/ [1 files][  147.0 B/  147.0 B]                                                \n",
      "Operation completed over 1 objects/147.0 B.                                      \n",
      "#!/usr/bin/python\n",
      "import pyspark\n",
      "sc = pyspark.SparkContext()\n",
      "rdd = sc.parallelize(['Hello,', 'world!'])\n",
      "words = sorted(rdd.collect())\n",
      "print words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py .\n",
    "!cat hello-world.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://hello-world.py [Content-Type=text/x-python]...\n",
      "-\n",
      "Operation completed over 1 objects/147.0 B.                                      \n",
      "Job [a22b76e0-3d65-4c5e-85fb-c96fec436e12] submitted.\n",
      "Waiting for job output...\n",
      "17/07/15 20:31:39 INFO org.spark_project.jetty.util.log: Logging initialized @2226ms\n",
      "17/07/15 20:31:39 INFO org.spark_project.jetty.server.Server: jetty-9.2.z-SNAPSHOT\n",
      "17/07/15 20:31:39 INFO org.spark_project.jetty.server.ServerConnector: Started ServerConnector@7fca9698{HTTP/1.1}{0.0.0.0:4040}\n",
      "17/07/15 20:31:39 INFO org.spark_project.jetty.server.Server: Started @2353ms\n",
      "17/07/15 20:31:39 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2\n",
      "17/07/15 20:31:40 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at test1-m/10.142.0.2:8032\n",
      "17/07/15 20:31:41 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1500148538199_0002\n",
      "['Hello,', 'world!']\n",
      "17/07/15 20:31:51 INFO org.spark_project.jetty.server.ServerConnector: Stopped ServerConnector@7fca9698{HTTP/1.1}{0.0.0.0:4040}\n",
      "Job [a22b76e0-3d65-4c5e-85fb-c96fec436e12] finished successfully.\n",
      "driverControlFilesUri: gs://dataproc-be4b433b-932d-4f29-842e-d0e896f67823-us/google-cloud-dataproc-metainfo/5241cd03-b0af-4013-8dac-6a252719650d/jobs/a22b76e0-3d65-4c5e-85fb-c96fec436e12/\n",
      "driverOutputResourceUri: gs://dataproc-be4b433b-932d-4f29-842e-d0e896f67823-us/google-cloud-dataproc-metainfo/5241cd03-b0af-4013-8dac-6a252719650d/jobs/a22b76e0-3d65-4c5e-85fb-c96fec436e12/driveroutput\n",
      "placement:\n",
      "  clusterName: test1\n",
      "  clusterUuid: 5241cd03-b0af-4013-8dac-6a252719650d\n",
      "pysparkJob:\n",
      "  loggingConfig: {}\n",
      "  mainPythonFileUri: gs://dataproc-be4b433b-932d-4f29-842e-d0e896f67823-us/google-cloud-dataproc-metainfo/5241cd03-b0af-4013-8dac-6a252719650d/jobs/a22b76e0-3d65-4c5e-85fb-c96fec436e12/staging/hello-world.py\n",
      "reference:\n",
      "  jobId: a22b76e0-3d65-4c5e-85fb-c96fec436e12\n",
      "  projectId: fiery-set-171213\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2017-07-15T20:31:56.201Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2017-07-15T20:31:33.866Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2017-07-15T20:31:34.368Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2017-07-15T20:31:34.853Z'\n",
      "yarnApplications:\n",
      "- name: hello-world.py\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://test1-m:8088/proxy/application_1500148538199_0002/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster test1 hello-world.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
